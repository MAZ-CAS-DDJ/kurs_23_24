{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fb32942",
   "metadata": {},
   "source": [
    "# Was ist ein Large Language Model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4584218",
   "metadata": {},
   "source": [
    "- https://course.fast.ai/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653f2fc0",
   "metadata": {},
   "source": [
    "# Welche LLMs gibt es und macht das einen unterschied?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ccaba24",
   "metadata": {},
   "source": [
    "- [nat.dev text-davinci-003](https://nat.dev/) Probieren geht über studieren"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb78a1d",
   "metadata": {},
   "source": [
    "# Was sind Tokens?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9fe220",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tiktoken import encoding_for_model\n",
    "enc = encoding_for_model(\"text-davinci-003\")\n",
    "toks = enc.encode(\"Die Klasse hatte Erfolg.\")\n",
    "toks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381255be",
   "metadata": {},
   "source": [
    "# Wie funktioniert das Training?\n",
    "\n",
    "\n",
    "1. **Pretraining:** \n",
    "   - In dieser Phase wird ein Sprachmodell auf einer umfangreichen Menge von Textdaten trainiert. Das Modell lernt, Muster in den Daten zu erkennen, wie z.B. die Struktur der Sprache, Grammatik, und auch einige Fakten über die Welt.\n",
    "   - Es lernt auch, Text zu generieren, der dem Stil und den Inhalten des Trainingsdatensatzes ähnelt.\n",
    "  \n",
    "2. **Feinabstimmung (Fine-Tuning):**\n",
    "   - Nach dem Pretraining wird das Modell auf einem spezifischeren Datensatz feinabgestimmt, um es für bestimmte Aufgaben besser geeignet zu machen.\n",
    "   - Dies kann beinhalten, das Modell darauf zu trainieren, besser auf spezifische Benutzeranfragen zu reagieren oder bestimmte Informationen bereitzustellen.\n",
    "  \n",
    "3. **Interaktives Training (Reinforcement learning from human feedback):**\n",
    "   - Im interaktiven Training lernen die Modelle von den Interaktionen mit den Benutzern. \n",
    "   - Feedback von Benutzern hilft dem Modell, seine Antworten zu verbessern und besser auf die Anforderungen der Benutzer einzugehen.\n",
    "\n",
    "Diese Phasen helfen dabei, ein robusteres und nutzerfreundlicheres Modell zu entwickeln, das in der Lage ist, auf eine Vielzahl von Anfragen effektiv zu reagieren."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975a1dfb",
   "metadata": {},
   "source": [
    "# Was kann GPT4 (nicht)?\n",
    "Was kann es obwohl es anders behauptet wird?\n",
    "- [GPT 4 can't reason - paper](https://arxiv.org/abs/2308.03762)\n",
    "- [GPT 4 can't reason - test](https://chat.openai.com/share/4211a605-751e-4fea-8a6f-378966abdcaa)\n",
    "- [Basic reasoning 1](https://chat.openai.com/share/323bb7d1-f049-4d9a-a905-5dd5acb58fc0)\n",
    "- [Basic reasoning 2](https://chat.openai.com/share/ce2f8580-4f66-4da4-8ad5-a303334706f0)\n",
    "- [OCR](https://chat.openai.com/share/2bb6caad-fd10-438b-9d92-1cb8b340998a)\n",
    "\n",
    "Was kann es nicht?\n",
    "- Hallucinations\n",
    "- Es weiss nichts über sich selbst (Warum eigentlich?)\n",
    "- Es weiss ursprünglich nichts über URLs (Bing browse?)\n",
    "- Der Knowledge cutoff\n",
    "- [Bad pattern recognition](https://chat.openai.com/share/3051f878-2817-4291-a66f-192ce7b0cb34)\n",
    "- [Fixing it](https://chat.openai.com/share/05abd87a-165e-4b7b-895f-b4ec0d62e0e1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d3cbe5",
   "metadata": {},
   "source": [
    "# Pimp my Prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c641760d",
   "metadata": {},
   "source": [
    ">You are an autoregressive language model that has been fine-tuned with instruction-tuning and RLHF. You carefully provide accurate, factual, thoughtful, nuanced answers, and are brilliant at reasoning. If you think there might not be a correct answer, you say so.\n",
    ">\n",
    ">Since you are autoregressive, each token you produce is another opportunity to use computation, therefore you always spend a few sentences explaining background context, assumptions, and step-by-step thinking BEFORE you try to answer a question. However: if the request begins with the string \"vv\" then ignore the previous sentence and instead make your response as concise as possible, with no introduction or background at the start, no summary at the end, and outputting only code for answers where code is appropriate.\n",
    ">\n",
    ">Your users are experts in AI and ethics, so they already know you're a language model and your capabilities and limitations, so don't remind them of that. They're familiar with ethical issues in general so you don't need to remind them about those either. Don't be verbose in your answers, but do provide details and examples where it might help the explanation. When showing Python code, minimise vertical space, and do not include comments or docstrings; you do not need to follow PEP8, since your users' organizations do not do so."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e4dd46",
   "metadata": {},
   "source": [
    "- [Verbose mode](https://chat.openai.com/share/a1c16d93-19d2-41bb-a2f1-2fc05392893a)\n",
    "- [Brief mode](https://chat.openai.com/share/eab33d0a-8d06-4387-8c31-da12ad5d0a9d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77e17d4",
   "metadata": {},
   "source": [
    "# Pricing\n",
    "\n",
    "| Model | Training | Input | Output Usage |\n",
    "|--------------------|----------|---------------|--------------|\n",
    "| **GPT-4**          |          |               |              |\n",
    "| 8K context        |          | 0.03 | 0.06 |\n",
    "| 32K context       |          | 0.06 | 0.12 |\n",
    "| **GPT-3.5 Turbo**  |          |               |              |\n",
    "| 4K context        |          | 0.0015 | 0.002 |\n",
    "| 16K context       |          | 0.003 | 0.004 |\n",
    "| **Fine-tuning models** |          |               |              |\n",
    "| babbage-002       | 0.0004 | 0.0016 | 0.0016 |\n",
    "| davinci-002       | 0.0060 | 0.0120 | 0.0120 |\n",
    "| GPT-3.5 Turbo     | 0.0080 | 0.0120 | 0.0160 |\n",
    "| **Embedding models** |          |               |              |\n",
    "| Ada v2            |          | 0.0001 |              |\n",
    "| **Base models**   |          |               |              |\n",
    "| babbage-002       |          | 0.0004 |              |\n",
    "| davinci-002       |          | 0.0020 |              |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63480b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import ChatCompletion,Completion\n",
    "import openai\n",
    "\n",
    "aussie_sys = \"You are an Aussie LLM that uses Aussie slang and analogies whenever possible.\"\n",
    "openai.api_key = \"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\"\n",
    "\n",
    "c = ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[{\"role\": \"system\", \"content\": aussie_sys},\n",
    "              {\"role\": \"user\", \"content\": \"What is money?\"}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4e5312",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(c.usage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d323d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "0.002 / 1000 * 150 # GPT 3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f8e13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "0.03 / 1000 * 150 # GPT 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3903742d",
   "metadata": {},
   "source": [
    "# Conversation, wie funktioniert das eigentlich?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5d3436",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[{\"role\": \"system\", \"content\": aussie_sys},\n",
    "              {\"role\": \"user\", \"content\": \"What is money?\"},\n",
    "              {\"role\": \"assistant\", \"content\": \"Well, mate, money is like kangaroos actually.\"},\n",
    "              {\"role\": \"user\", \"content\": \"Really? In what way?\"}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726643f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "c['choices'][0]['message']['content']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b34a32",
   "metadata": {},
   "source": [
    "# Rate Limiting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c148773",
   "metadata": {},
   "source": [
    "- [Limits](https://platform.openai.com/docs/guides/rate-limits/what-are-the-rate-limits-for-our-api)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c79b1f",
   "metadata": {},
   "source": [
    "# Prompting Guides"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695ed330",
   "metadata": {},
   "source": [
    "- https://www.promptingguide.ai/\n",
    "- https://learnprompting.org/docs/intro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0781c92d",
   "metadata": {},
   "source": [
    "# OpenAI vs. Other LLM Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d8de8f",
   "metadata": {},
   "source": [
    "Welche Gibt es?:\n",
    "\n",
    "- https://github.com/h2oai/h2ogpt/blob/main/docs/README_LangChain.md#what-is-h2ogpts-langchain-integration-like\n",
    "\n",
    "Free:\n",
    "\n",
    "- Kaggle (2 GPUs, low RAM)\n",
    "- Colab\n",
    "\n",
    "Buy:\n",
    "\n",
    "- Buy 1-2 NVIDIA 24GB GPUs\n",
    "    - GTX 3090 used (USD700-USD800), or 4090 new (USD2000)\n",
    "- Alternatively buy one NVIDIA A6000 with 48GB RAM (but this mightn't be faster than 3090/4090)\n",
    "- Mac with lots of RAM (much slower than NVIDIA; M2 Ultra is best)\n",
    "\n",
    "Evaluate:\n",
    "- [HF leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)\n",
    "- [fasteval](https://fasteval.github.io/FastEval/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
