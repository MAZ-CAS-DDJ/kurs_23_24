{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Könnnen wir darstellen worüber USA Kandidaten auf Twitter reden?\n",
    "\n",
    "Im Grunde wollen wir ein bisschen das hier nachbauen https://www.bloomberg.com/graphics/2020-democratic-presidential-candidate-policies/\n",
    "\n",
    "Agenda:\n",
    "\n",
    "- Topic Modeling als Inspiration für Themen\n",
    "- Themen anhand von Keywords anlegen\n",
    "- Steamgraphs machen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option(\"display.max_columns\", 60)\n",
    "pd.set_option(\"display.max_colwidth\", 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweets besorgen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Ich habe 2021 39 Tausend Tweets aus Twitter Accounts folgender Demokraten heruntergeladen.  Ich habe [GetOldTweets3](https://github.com/Mottl/GetOldTweets3/) benutzt. \n",
    "- 2022 schien es Probleme zu machen. Das ist leider normal bei solchen scrapern. Dieses Jahr funktionierte [twint](https://github.com/twintproject/twint). \n",
    "- Dieses Jahr scheint [TWMD](https://github.com/mmpx12/twitter-media-downloader) zu funktionieren. \n",
    "Es ist ein lost Battle :) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usernames = [\n",
    "    'joebiden', 'corybooker','petebuttigieg','juliancastro','kamalaharris',\n",
    "    'amyklobuchar','betoorourke','berniesanders','ewarren','andrewyang',\n",
    "    'michaelbennet','governorbullock','billdeblasio','johndelaney',\n",
    "    'tulsigabbard','waynemessam','timryan','joesestak','tomsteyer',\n",
    "    'marwilliamson','sengillibrand','hickenlooper','jayinslee',\n",
    "    'sethmoulton','ericswalwell'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Schauen wir mal rein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We don't need all of the columns, let's leave out a lot of them\n",
    "columns = ['username', 'text', 'date']\n",
    "df = pd.read_csv(\"data/tweets.csv\", usecols=columns)\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wie viele haben wir genau?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wie viele von jedem Kandidaten?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.username.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topics mit Keywords selbst machen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir erstellen eine kurze Liste von Wörtern , die mit dem Thema verbunden sind. Unser Ansatz wird etwas umständlich sein, aber er ist ziemlich flexibel für Dinge, die wir in Zukunft vielleicht tun möchten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = {\n",
    "    'immigration': ['immigration', 'border', 'wall'],\n",
    "    'education': ['students', 'education', 'teacher'],\n",
    "    'foreign_policy': ['foreign policy', 'peace'],\n",
    "    'climate_change': ['climate', 'emissions', 'carbon'],\n",
    "    'economy': ['economy', 'tariffs', 'taxes'],\n",
    "    'military': ['veterans', 'troops', 'war'],\n",
    "    'jobs': ['jobs', 'unemployment', 'wages'],\n",
    "    'drugs': ['drugs', 'opioid'],\n",
    "    'health': ['health', 'insurance', 'medicare'],\n",
    "    'repro_rights': ['reproductive', 'abortion'],\n",
    "    'gun_control': ['gun'],\n",
    "}\n",
    "categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir werden diese in einen schönen langen Dataframe aus Wörtern und Kategorienamen verwandeln. \n",
    "Wir werden auch die Keywords stemmen, damit sie etwas breiter übereinstimmen: `immigrant`, `immigrants`, and `immigration` sollten alle als  `immigr` gestemmt werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "dfs = []\n",
    "for key,values in categories.items():\n",
    "    for word in values:\n",
    "        words = {'category': key, 'term': stemmer.stem(word)}\n",
    "        dfs.append(words)\n",
    "\n",
    "terms_df = pd.DataFrame(dfs)\n",
    "terms_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count Vectorizer mit Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jetzt werden wir unseren eignen Vektorisierer machen: **Wir werden nur Wörter in unserer Liste zählen** und wir werden nur Ja / Nein (1/0) dh. existiert bzw. existiert nicht für jedes der Wörter aufschreiben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This document is the second document.',\n",
    "    'And this is the third one.',\n",
    "    'Is this the first document?',\n",
    "]\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(vectorizer.get_feature_names_out())\n",
    "print(X.toarray())\n",
    "pd.DataFrame(X.toarray(),columns=vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    stems = [stemmer.stem(token) for token in tokens]\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_list = list(terms_df[\"term\"])\n",
    "term_list[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(tokenizer=tokenize,binary=True,vocabulary=term_list) # binary=True macht nur 0/1 # vocabulary= das Vokabular das wir tracken wollen\n",
    "matrix = vectorizer.fit_transform(df.text)\n",
    "words_df = pd.DataFrame(matrix.toarray(),columns=vectorizer.get_feature_names_out())\n",
    "words_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Von den ersten fünf Tweets scheint nur zeile 3 in eine Kategorie zu passen - er enthält das Wort **teacher**, also werden wir es in die Bildungskategorie (education) aufnehmen.\n",
    "\n",
    "Wir werden jede Kategorie durchlaufen und dann prüfen, ob einer der Begriffe für diese Kategorie eine \"1\" enthält. In diesem Fall weisen wir dieser Zeile eine \"1\" für die Kategorie zu. Wenn nicht, geben wir eine 0.\n",
    "\n",
    "Zum Beispiel hat Zeile 3 nicht \"students\" oder \"education\", sondern \"teacher\". Als Ergebnis erhält es eine \"1\" für die Bildungskategorie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = ['climat', 'emiss', 'carbon']\n",
    "words_df[terms]#.any(axis=1).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Von stemmed words wieder zurück zu Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the terms by category, then loop through each category\n",
    "for category_name, rows in terms_df.groupby('category'):\n",
    "    # Wandle die begriffe für eine Kategorie in eine einfache liste um z.B. ['student', 'educ', 'teacher']\n",
    "    terms = list(rows['term'])\n",
    "    print(f\"Looking at {category_name} with terms {terms}\")\n",
    "\n",
    "    # words_df[terms] holt alle columns für 'student', 'educ', und 'teacher'\n",
    "    # .any(axis=1) schaut ob eine davon eine 1 ist , weisst dann True/False zu\n",
    "    # .astype(int) wandelt True/False in 1/0 um\n",
    "    # df[category_name] = weist dann den wert df['education'] zu\n",
    "    df[category_name] = words_df[terms].any(axis=1).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nach Person gruppieren\n",
    "\n",
    "Nachdem wir nun eine Reihe von Tweets haben, die mit verschiedenen Kategorien gekennzeichnet sind, können wir beginnen, sie zu zählen und zu klassifizieren. Zum Beispiel können wir sehen, wer am meisten über Jobs twittert. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('username').drugs.sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nach allen Themen gruppieren\n",
    "\n",
    "Da diese 0 und 1 unsere einzigen numerischen Spalten sind, können wir den Dataframe einfach auffordern, nach Benutzernamen zu gruppieren und jede Kategorie zu addieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df.drop(columns=[\"topic_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall = df.groupby('username').sum()\n",
    "overall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das Problem mit dieser Ansicht ist, dass einige Kandidaten viel twittern und einige Kandidaten viel weniger. Wenn wir es grafisch darstellen, gibt es keinen guten Überblick darüber, welche Themen die Kampagnen der Kandidaten wertschätzen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "ax = overall.plot(kind='bar', stacked=True, figsize=(13,6), width=0.9)\n",
    "\n",
    "# Move the legend off of the chart\n",
    "ax.legend(loc=(1.04,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Was wir brauchen ist eine Ansicht, die auf Prozentsätzen basiert. Dazu müssen wir jede Spalte durch die Summe der Zählungen in dieser Spalte teilen. Wegen \"Pandas-Logik\" müssen wir \".div\" anstelle von \"/\" verwenden. Das Teilungszeichen gibt euch möglicherweise keinen Fehler, aber es gibt definitiv falsche Ergebnisse!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_pct = overall.div(overall.sum(axis=1), axis=0)\n",
    "overall_pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = overall_pct.plot(kind='bar', stacked=True, figsize=(13,6), width=0.9)\n",
    "\n",
    "# Move the legend off of the chart\n",
    "ax.legend(loc=(1.04,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mit Plotly visualisieren\n",
    "Matplotlib sieht oft ewas hässlich aus. Lasst uns lieber plotly nehmen, das ist schön interaktiv. \n",
    "Wir müssen dafür den Dataframe etwas re-shapen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reshaped = overall_pct.reset_index().melt(id_vars=['username'], var_name='topic', value_name='pct')\n",
    "reshaped.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "fig = px.bar(reshaped, x='username', y='pct', color='topic')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steamgraphs machen\n",
    "\n",
    "Streamgraphs sind eine Visualisierungstechnik, die nicht allzu oft verwendet wird, aber ich glaube die Leute mögen sie. Es ist eine Art gestapeltes Flächendiagramm, das vertikal zentriert ist und eine Handvoll Vor- und Nachteile enthält. \n",
    "\n",
    "Wir müssen zunächst das Datum typcasten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the date to a datetime, then pull out the week\n",
    "df['date'] = pd.to_datetime(df.date)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir sehen jetzt, dass es ein datetime64 [ns, UTC] ist, was bedeutet, dass wir Dinge tun können, wie den Tag des Jahres oder den Monat oder die Woche herauszuholen! Wir wollen unsere Daten jedoch nach jeder Woche gruppieren."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das Gruppieren nach Zeit heisst resampling auf und ist recht einfach. Wir werden die Tweets von Kamala Harris herausziehen und sie dann anhand der Datumsspalte in 8-Tage-Chunks resamplen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.username == 'BernieSanders']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample and make it sum every 7 days\n",
    "harris = df[df.username == 'BernieSanders'].resample('8D', on='date').sum()\n",
    "harris.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Als Grafik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = harris.plot(kind='area', stacked=True)\n",
    "\n",
    "# Move the legend off of the chart\n",
    "ax.legend(loc=(1.04,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steamgraphs mit Matplotlib\n",
    "\n",
    "So wie bekommen wir Steamgraphs hin, gottseidank hat Matplotlib ein Beispiel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "\n",
    "# Plot a stackplot - https://matplotlib.org/3.1.1/gallery/lines_bars_and_markers/stackplot_demo.html\n",
    "ax.stackplot(harris.index, harris.T, baseline='wiggle', labels=harris.columns)\n",
    "\n",
    "# Move the legend off of the chart\n",
    "ax.legend(loc=(1.04,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sieht ok aus aber so zackig. Wir können nicht einfach sagen: \"Zeichne glatte Linien!\" Matplotlib benötigt echte Daten. Es ist sehr zackig, weil die Daten derzeit nur alle acht Tage vorliegen und jedees Mal ein sehr scharfer Sprung zum nächsten sein kann."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Glätten mit Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir tun so, als hätten wir alle zwei Tage Daten.  Zuerst machen wir eine Liste aller Tage, die wir existieren wollen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a list of dates between the first and last \n",
    "first = harris.index.min()\n",
    "last = harris.index.max()\n",
    "\n",
    "# Go between the first and the last in 2-day chunks\n",
    "frequency = pd.date_range(start=first, end=last, freq='2D')\n",
    "frequency[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jetzt werden wir sie unserem Dataframe hinzufügen. Die Daten werden fehlen, wenn wir die neuen Zeilen hinzufügen, da Pandas sicher sein kann was dort hin soll."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reindex our dataframe, adding a bunch of new days, but missing data!\n",
    "smooth = harris.reindex(frequency)\n",
    "smooth.head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Füllen wir diese Daten durch Interpolation aus. Wir werden Pandas sagen, dass er eine quadratische Interpolation verwenden soll, um es schön und glatt zu machen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plan out 2-day chunks between the first and last days\n",
    "first = harris.index.min()\n",
    "last = harris.index.max()\n",
    "frequency = pd.date_range(start=first, end=last, freq='2D')\n",
    "\n",
    "# Inject the new (empty) rows, then interpolate new data\n",
    "smoothed = harris.reindex(frequency).interpolate(method='quadratic')\n",
    "smoothed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Das Finale\n",
    "So jetzt probieren wir es nochmal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "\n",
    "# Plot a stackplot - https://matplotlib.org/3.1.1/gallery/lines_bars_and_markers/stackplot_demo.html\n",
    "ax.stackplot(smoothed.index, smoothed.T,\n",
    "             baseline='wiggle', labels=smoothed.columns)\n",
    "\n",
    "# Move the legend off of the chart\n",
    "ax.legend(loc=(1.04,0))\n",
    "\n",
    "# Set the title\n",
    "ax.set_title(\"Bernie twitter topics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In hübsch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('ggplot')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
