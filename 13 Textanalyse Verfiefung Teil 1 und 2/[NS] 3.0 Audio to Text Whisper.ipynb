{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI's Whisper Speech Recognition Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](whisper.jpg \"Intro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lbja1jB3vDOT"
   },
   "source": [
    "\n",
    "Whisper ist der Name eines automatischen Spracherkennungssystems (ASR) von OpenAI. ASR-Systeme wandeln gesprochene Sprache in geschriebenen Text um. Sie werden in einer Vielzahl von Anwendungen eingesetzt, darunter Sprachassistenten, Transkriptionsdienste und mehr.\n",
    "\n",
    "Whisper wurde auf einer großen Menge multilingualer und multitaskaler Daten aus dem Web trainiert. Es wurde entwickelt, um in verschiedenen Anwendungen und Kontexten eine hohe Genauigkeit bei der Spracherkennung zu bieten. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kosakhNmxb7A"
   },
   "source": [
    "## Install Whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZsJUxc0aRsAf",
    "outputId": "9c23b51f-3832-4856-b90d-71fd01916026"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai-whisper in /Users/plotti/miniconda3/envs/chat/lib/python3.9/site-packages (20231106)\n",
      "Requirement already satisfied: more-itertools in /Users/plotti/miniconda3/envs/chat/lib/python3.9/site-packages (from openai-whisper) (10.1.0)\n",
      "Requirement already satisfied: torch in /Users/plotti/miniconda3/envs/chat/lib/python3.9/site-packages (from openai-whisper) (2.0.1)\n",
      "Requirement already satisfied: numpy in /Users/plotti/miniconda3/envs/chat/lib/python3.9/site-packages (from openai-whisper) (1.25.2)\n",
      "Requirement already satisfied: tqdm in /Users/plotti/miniconda3/envs/chat/lib/python3.9/site-packages (from openai-whisper) (4.65.0)\n",
      "Requirement already satisfied: tiktoken in /Users/plotti/miniconda3/envs/chat/lib/python3.9/site-packages (from openai-whisper) (0.3.3)\n",
      "Requirement already satisfied: numba in /Users/plotti/miniconda3/envs/chat/lib/python3.9/site-packages (from openai-whisper) (0.58.0)\n",
      "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /Users/plotti/miniconda3/envs/chat/lib/python3.9/site-packages (from numba->openai-whisper) (0.41.0)\n",
      "Requirement already satisfied: requests>=2.26.0 in /Users/plotti/miniconda3/envs/chat/lib/python3.9/site-packages (from tiktoken->openai-whisper) (2.29.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/plotti/miniconda3/envs/chat/lib/python3.9/site-packages (from tiktoken->openai-whisper) (2023.10.3)\n",
      "Requirement already satisfied: networkx in /Users/plotti/miniconda3/envs/chat/lib/python3.9/site-packages (from torch->openai-whisper) (3.1)\n",
      "Requirement already satisfied: sympy in /Users/plotti/miniconda3/envs/chat/lib/python3.9/site-packages (from torch->openai-whisper) (1.12)\n",
      "Requirement already satisfied: typing-extensions in /Users/plotti/miniconda3/envs/chat/lib/python3.9/site-packages (from torch->openai-whisper) (4.8.0)\n",
      "Requirement already satisfied: filelock in /Users/plotti/miniconda3/envs/chat/lib/python3.9/site-packages (from torch->openai-whisper) (3.12.4)\n",
      "Requirement already satisfied: jinja2 in /Users/plotti/miniconda3/envs/chat/lib/python3.9/site-packages (from torch->openai-whisper) (3.1.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/plotti/miniconda3/envs/chat/lib/python3.9/site-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/plotti/miniconda3/envs/chat/lib/python3.9/site-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.1.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/plotti/miniconda3/envs/chat/lib/python3.9/site-packages (from requests>=2.26.0->tiktoken->openai-whisper) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/plotti/miniconda3/envs/chat/lib/python3.9/site-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2023.7.22)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/plotti/miniconda3/envs/chat/lib/python3.9/site-packages (from jinja2->torch->openai-whisper) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/plotti/miniconda3/envs/chat/lib/python3.9/site-packages (from sympy->torch->openai-whisper) (1.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install openai-whisper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nur ausführen wenn ihr auf einem Colab notebook seid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone git@github.com:MAZ-CAS-DDJ/kurs_23_24.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AtAvuKSJxhNw"
   },
   "source": [
    "## Modell laden\n",
    "\n",
    "- Das sollte ca. 140 MB runterladen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121,
     "referenced_widgets": [
      "d0398a17068742168da49ace4d565c8b",
      "58b20a8e2b95488185676aaa323cb6b5",
      "b86c7de5a9fa4c6dabf642c5ac581acc",
      "20193b33576947e18a024b2aef896a0b",
      "826d6c16641c47d59dfe7d75d7b5d3ab",
      "89802a797502466f8e1b8ecec6cb3b21",
      "0042cf3f78d441f69d812964347457da",
      "c93aa11d360545d2bd41c7cf31109e80",
      "392c6c44e3784d39a7bf34fb5fbc6625",
      "cd8b621110b5429b8cacfeeab208fa17",
      "843cb01b868d483cbc4ce56484d84922"
     ]
    },
    "id": "Kr5faKybKi4p",
    "outputId": "62d32a9f-2e4d-4585-8b2e-e26deb08d791"
   },
   "outputs": [],
   "source": [
    "import whisper\n",
    "import numpy as np\n",
    "model = whisper.load_model(\"base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e200RNNlxn5j"
   },
   "source": [
    "## Können wir die GPU nutzen?\n",
    "\n",
    "Auf der GPU vs. CPU arbeiten. Im besten Fall steht dort device GPU, das hängt von eurem Laptop ab. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u_6_s2iHboR4",
    "outputId": "16e80d4b-57f3-47b7-c8df-b1a8972bc1da"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QwLTZtubySoo"
   },
   "source": [
    "## Define the Transcribe Function\n",
    "\n",
    "Jetzt haben wir das Modell geladen und haben den Code. Dies ist die Funktion, die einen Audiodateipfad als Eingabe verwendet und den erkannten Text zurückgibt (und protokolliert, was seiner Meinung nach die Sprache ist)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!brew install ffmpeg\n",
    "#/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "JtTvvQQPcOZZ"
   },
   "outputs": [],
   "source": [
    "CHUNK_LIM = 480000 # On my system....\n",
    "\n",
    "def transcribe(audio):\n",
    "\n",
    "    # load audio and pad/trim it to fit 30 seconds\n",
    "    audio = whisper.load_audio(audio)\n",
    "    audios = []\n",
    "\n",
    "    if len(audio) <= CHUNK_LIM:\n",
    "        audios.append(audio)\n",
    "    else:\n",
    "        for i in range(0, len(audio), CHUNK_LIM):\n",
    "            chunk = audio[i:i + CHUNK_LIM]\n",
    "            chunk_index = len(chunk)\n",
    "            if chunk_index < CHUNK_LIM:\n",
    "                padding = [0] * (CHUNK_LIM - chunk_index)\n",
    "                array1 = np.array(chunk)\n",
    "                array2 = np.array(padding)\n",
    "                concat =  np.concatenate((array1, array2))\n",
    "                chunk = concat.astype(np.float32)\n",
    "            audios.append(chunk)\n",
    "\n",
    "    results = \"\"\n",
    "\n",
    "    for chunk in audios:\n",
    "\n",
    "        # make log-Mel spectrogram and move to the same device as the model\n",
    "        mel = whisper.log_mel_spectrogram(chunk).to(model.device)\n",
    "\n",
    "        # decode the audio\n",
    "        # bei M1 muss hier fp16 = False stehen, das müsst ihr nicht unbeding übernehmen\n",
    "        options = whisper.DecodingOptions(fp16=False)\n",
    "        result = whisper.decode(model, mel, options)\n",
    "        results += result.text\n",
    "\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_QRwhuOXynZQ"
   },
   "source": [
    "## Test mit High Quality Audio\n",
    "\n",
    "Ihr werdet feststellen, dass das zweite Transkript normalerweise nach 30 Sekunden abgeschnitten wird. Dies ist die Standardlänge. Diese kann erweitert werden. Siehe oben, mit chunking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bDXgLIprIsAj",
    "outputId": "0a050f6a-b7c4-4e35-b4a1-32198ed58212"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tazy, Tazy, Tazy. Give me your answer to time after crazy all for the love of you. It won't be a stylish marriageGod, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh\n"
     ]
    }
   ],
   "source": [
    "#easy_text = transcribe(\"mary.mp3\")\n",
    "#print(easy_text)\n",
    "\n",
    "hard_text = transcribe(\"daisy_HAL_9000.mp3\")\n",
    "print(hard_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ojRF2zWzcYa"
   },
   "source": [
    "## Selbst ausprobieren mit dem Web UI Toolkit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fjM27tWsI4dH",
    "outputId": "8a306b8b-4fa0-4056-ee9b-c2396f243a37"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install gradio -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "ILFOYNnTcYe8"
   },
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2tHwfOG-zlY1"
   },
   "source": [
    "## Web Interface\n",
    "\n",
    "After running this script, you should see two widgets below that you can use to record live audio and see the transcription, as described in the introduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 767
    },
    "id": "deSAVvfJcWBo",
    "outputId": "eee80d4b-3461-42d9-c29f-1b992935c1a2"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'gradio' has no attribute 'inputs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m gr\u001b[38;5;241m.\u001b[39mInterface(\n\u001b[1;32m      2\u001b[0m     title \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOpenAI Whisper ASR Gradio Web UI\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      3\u001b[0m     fn\u001b[38;5;241m=\u001b[39mtranscribe,\n\u001b[1;32m      4\u001b[0m     inputs\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m----> 5\u001b[0m         \u001b[43mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[38;5;241m.\u001b[39mAudio(source\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmicrophone\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfilepath\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m     ],\n\u001b[1;32m      7\u001b[0m     outputs\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m      8\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtextbox\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      9\u001b[0m     ],\n\u001b[1;32m     10\u001b[0m     live\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mlaunch()\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'gradio' has no attribute 'inputs'"
     ]
    }
   ],
   "source": [
    "gr.Interface(\n",
    "    title = 'OpenAI Whisper ASR Gradio Web UI',\n",
    "    fn=transcribe,\n",
    "    inputs=[\n",
    "        gr.inputs.Audio(source=\"microphone\", type=\"filepath\")\n",
    "    ],\n",
    "    outputs=[\n",
    "        \"textbox\"\n",
    "    ],\n",
    "    live=True).launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download a youtube video, transcode it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!yt-dlp \"https://www.youtube.com/watch?v=ww0zvl18-Ys\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "charlie = transcribe(\"Charlie Chaplins Rede an die Menschheit [ww0zvl18-Ys].webm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Es tut mir leid, aber ich möchte nun mal kein Herrscher der Welt sein, denn das liegt mir nicht. Ich möchte weder Herrchen noch irgendwie erobern, sondern jede Menschen helfen, wo immer ich kann, den Juden, den Heiden, den Fahrwigen, den Weißen. Jeder Mensch sollte dem anderen helfen, nur so verbessern wir die Welt. Wir sollten am Glück des anderen Teilhaben und nicht an der Verabschollen. Pass und Verachtung bringen uns niemals näher. Auf dieser Welt ist Platz genug für jeden und Mutter Erde ist reich genug, um jeden von uns Satz zu machen.Wir müssen es nur wieder zu leben lernen. Die Habki hat das Gute im Menschen verschützt. Und Missgunst hat die Seelen vergiftet und uns im Parade-Schritt zu verdärb und Blutschuld geführt. Wir haben die Geschwindigkeit entwickelt, aber innerlich sind wir stehen geblieben. Wir lassen Maschinen für uns arbeiten und sie denken auch für uns. Die Glugheit hat uns hochmütig werden lassen und unser Wissen kalt und hart. Wir sprechen zu viel und fühlen zu wenig. Aber zuerst kommt die Menschlichkeit und dann erst die Maschinen. Vor Glugheit und Wissen kommt Toleranz und Güte.Menschen, die die Menschen und die nächsten Liebe ist, unser Dasein nicht lebenswert. Eroplane und Radio haben uns ein Andernäher gebracht. Die Erfindungen haben eine Brücke geschlagen von Mensch zu Mensch. Die erforderne eine allumfassende Brüderlichkeit, damit wir alle einswerten. Millionen Menschen auf der Welt können im Augenblick meine Stimme hören. Millionen verzweifelte Menschen, ob für eine System, dass es sich zur Aufgabe gemacht hat, unschuldige zu quälen und im Ketten zu legen. Allen denen, die mich jetzt hören, ruf ich zu, ihr dürft nicht verzagen.Die Männer, die heute die Menschlichkeit mit Füßen treten, werden nicht immer da sein. Ihre Grausamkeit stirbt mit ihnen und auch ihr Hass. Die Freiheit, die sie den Menschen genommen haben, werden ihnen dann zu Höcke geben werden. Auch wenn es Blut und Reinen kostet. Für die Freiheit ist kein Opfer zu groß. Soldaten, vertraut euch nicht barbaren. An unmenschen die euch verachten und denen euer Leben nichts wert ist. Ihr seid für sie nur Sklaven. Ihr habt das zu tun, das zu glauben, das zu fühlen. Ihr werdet jetzt hält, gefüttert, wie viel behandelt und seid nicht zweit als Kanon-Futter.Er ist ein verdienter Erdensobjekt. Diese Maschine, mit Maschinen, Kratzen, und Maschinen, Herzen. Ihr seid keine Roboter, ihr seid keine Tiere, ihr seid Menschen. Er wartet euch die Menschlichkeit in euren Herzen und hast nicht. Nur wenn nicht geliebt wird, hast, nur wenn nicht geliebt wird. Säuter, kämpft nicht für die Sklaverei, kämpft für die Freiheit. Im 17. Kapitel, das Evangelisten, Lukas steht Gott, wo und in jede Mensch. Also nicht nur in einem Urleine Gruppe von Menschen. Herr der Sneegot lebt in euch allen und ihr als Volk habt.Hier als Volkabtes in der Hand dieses Leben einmalig kostbar zu machen. Es wird wunderbar im Freiheitsgeiz zu den Tringen. Da ihr im Namen der Demokratie. Lasst uns diese Macht nutzen. Lasst uns zusammenstehen. Lasst uns kämpfen für eine neue Welt. Für eine Anständige Welt. Wie jeder haben gleiche Schossen gibt. Wie der Jugend in Zukunft und in alten Sicherheit gewährt. Er sprach und haben die Order deugert das auch. Deshalb kommt die Macht da rein. Das war rüge wie überhaupt. Alles was ihr euch versparen, diese Paperecher. Diktator und wollen die Freiheit nur für sich. Das Volk soll was verbleiben.Ich komme zu einem, lass uns kämpfen für eine bessere Welt. Lass uns kämpfen für die Freiheit in der Welt. Was ist mein Ziel, für das sich zu kämpfen los? Nieder mit der Unterkunft im Hass und der Intoleran. Lass uns kämpfen für eine Welt der Sauberkeit. In der die Vernunft zieht, in der Ortschett und Wissenschaft uns alle zu sehen. Kann man ja in Namen der Demokratie. Dafür lass uns steigen.'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "charlie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "charlie = transcribe(\"tarczynski.webm\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0042cf3f78d441f69d812964347457da": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "20193b33576947e18a024b2aef896a0b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cd8b621110b5429b8cacfeeab208fa17",
      "placeholder": "​",
      "style": "IPY_MODEL_843cb01b868d483cbc4ce56484d84922",
      "value": " 0/0 [00:00&lt;?, ?it/s]"
     }
    },
    "392c6c44e3784d39a7bf34fb5fbc6625": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "58b20a8e2b95488185676aaa323cb6b5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_89802a797502466f8e1b8ecec6cb3b21",
      "placeholder": "​",
      "style": "IPY_MODEL_0042cf3f78d441f69d812964347457da",
      "value": ""
     }
    },
    "826d6c16641c47d59dfe7d75d7b5d3ab": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "843cb01b868d483cbc4ce56484d84922": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "89802a797502466f8e1b8ecec6cb3b21": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b86c7de5a9fa4c6dabf642c5ac581acc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c93aa11d360545d2bd41c7cf31109e80",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_392c6c44e3784d39a7bf34fb5fbc6625",
      "value": 0
     }
    },
    "c93aa11d360545d2bd41c7cf31109e80": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "cd8b621110b5429b8cacfeeab208fa17": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d0398a17068742168da49ace4d565c8b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_58b20a8e2b95488185676aaa323cb6b5",
       "IPY_MODEL_b86c7de5a9fa4c6dabf642c5ac581acc",
       "IPY_MODEL_20193b33576947e18a024b2aef896a0b"
      ],
      "layout": "IPY_MODEL_826d6c16641c47d59dfe7d75d7b5d3ab"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
