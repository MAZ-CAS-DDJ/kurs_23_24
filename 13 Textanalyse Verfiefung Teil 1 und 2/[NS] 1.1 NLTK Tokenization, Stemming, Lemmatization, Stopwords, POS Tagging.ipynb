{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arbeiten mit Text mit NLTK, Textblob und Spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wenn ihr vorher Zeit habt installiert schon mal folgende Pakete:\n",
    "- ```!pip3 install spacy```\n",
    "- ```pip install -U textblob```\n",
    "- ```pip install -U textblob-de```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.9.12\r\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install spacy\n",
    "#!pip3 install -U textblob\n",
    "#!pip3 install -U textblob-de"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agenda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tokenization\n",
    "- Stemming\n",
    "- Lemmatization\n",
    "    - Pattern\n",
    "    - TextBlob\n",
    "- POS Tagging\n",
    "    - Pattern\n",
    "    - TextBlob\n",
    "- Stop Word removal\n",
    "- Wordle 2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Was ist NLTK\n",
    "\n",
    "- Natural Language Toolkit ist ein sehr praktisches python tool um mit Text zu arbeiten. Früher der quasi standard für viele Aufgaben. \n",
    "- Es beinhaltet viele Sprachmodelle und Textsammlungen sog. Corpora.\n",
    "```nltk.download()```\n",
    "- http://www.nltk.org\n",
    "- Es gibt auch ein öffentlich verfügbares freies Buch zu nltk online https://www.nltk.org/book\n",
    "- ```pip install nltk```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!conda install nltk -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wir werden NLTK nutzen um ein paar wichtige Begriffe aus der Textanalyse kennenzulernen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer\n",
    "- Wie splittet man einfach einen Satz in Wörter auf?\n",
    "- https://www.admin.ch/opc/de/classified-compilation/19995395/index.html#a8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artikel = '''Telefonbetrug: «Auch 20- oder 30-Jährige können Opfer werden»\n",
    "Telefonbetrüger werden zunehmend perfider. Und persönlicher – indem sie Social-Media-Profile abgrasen. Gefeit davor sind auch Junge nicht.\n",
    "85\n",
    "03.10.2023, 03:0003.10.2023, 10:44\n",
    "Anna Kappeler\n",
    "Anna Kappeler\n",
    "Folge mir\n",
    "Mehr «Gesellschaft & Politik»\n",
    "9-Millionen-Schweiz: So unterschiedlich sind die Auswirkungen in den Regionen\n",
    "Neuste Umfrage zu den Wahlen: Die SVP legt noch mehr zu – Grüne und FDP brechen ein\n",
    "Neuste Umfrage zu den Wahlen: Trends unverändert – aber die SVP legt noch mehr zu\n",
    "Machtkampf im US-Repräsentantenhaus: McCarthys Absetzung beantragt\n",
    "Promotion\n",
    "Warum du in SwissShrimp investieren solltest\n",
    "Promotion\n",
    "Bahngenuss im Bernina Express - einmalig schön, einmalig günstig\n",
    "\n",
    "Über zweihunderttausend Franken. So viel hat ein Telefonbetrüger Ende September ergaunert. Und das von einer 34-Jährigen, wie die Kantonspolizei Zürich diesen Montag mitgeteilt hat.\n",
    "\n",
    "Mehrmals täglich hat ein Unbekannter die Frau aus dem Bezirk Horgen angerufen. Er gab sich als Mitarbeiter von Interpol aus. Ihre Personalien seien gestohlen und damit diverse Betrugsdelikte begangen worden, behauptete dieser. Nun drohe ihr Gefängnis. Ausser, sie arbeite mit den Behörden zusammen und überweise Geld auf ein «sicheres Konto».\n",
    "8 Millionen Franken ergaunert\n",
    "\n",
    "Die Frau ist kein Einzelfall. Über acht Millionen Franken haben Telefonbetrüger 2023 bereits ergaunert, wie die Schweizerische Kriminalprävention SKP schreibt. Das sind über 2800 Schockanrufe oder Enkeltrickbetrüge – dreimal so viele Fälle wie im letzten Jahr.\n",
    "4 von 5 Älteren erlebten Betrugsversuch – die wahre Falle liegt aber nicht beim Enkeltrick\n",
    "\n",
    "Beunruhigend: «Jüngere sind ebenfalls betroffen, wenngleich der Fokus auf Senioren liegt», sagt Beatrice Kübli, Projektleiterin bei der Schweizerischen Kriminalprävention SKP, zu watson. «Auch 20- oder 30-Jährige können Opfer werden.»\n",
    "«Jüngere sind ebenfalls betroffen, wenngleich der Fokus auf Senioren liegt.»\n",
    "Daten im Netz als Infos\n",
    "\n",
    "Die Betrüger seien oft gut vorbereitet und «haben zuvor beispielsweise via Social Media über die Person recherchiert». So wüssten sie etwa, wenn die Eltern gerade im Ausland in den Ferien seien. «Und das nutzen sie aus», sagt Kübli.\n",
    "\n",
    "Wir alle hinterliessen so viele Daten im Netz, dass es für Betrüger ein Leichtes sei, an persönliche Informationen zu kommen. Besonders verbreitet sei der sogenannte Schockanruf. «Die Betrüger geben sich etwa als Chefarzt einer Klinik aus und wollen Geld für eine Notoperation der verunfallten Angehörigen.» Die Betrugsgeschichten gleichen sich, sagt Kübli, «egal ob bei jung oder alt». Je nach Situation variiere die Person in vermeintlicher Not, mal sei es die Mutter, mal der Sohn.\n",
    "\n",
    "«Und ja, das funktioniert auch bei Jungen», sagt Kübli. Im Schock und im Ausnahmezustand sind laut Kübli alle leichtgläubiger und denken weniger rational. Doch sie würden weniger häufig reingelegt. «Generell gilt: Junge lassen sich von Autoritäten weniger schnell einschüchtern», sagt Kübli. «Ein Anruf von einem Chefarzt oder Staatsanwalt beeindruckt eine 35-Jährige weniger als eine 70-Jährige.»\n",
    "«Hoi Mami, ich habe mein Handy verlegt»\n",
    "\n",
    "Es begann im Juni mit einer Nachricht auf dem Messenger Signal, erinnert sich die 69-jährige Monika Egli aus Zürich. Die Nummer: unbekannt. Auf Schweizerdeutsch stand sinngemäss:\n",
    "\n",
    "«Hoi Mami, ich habe mein Handy verlegt. Kannst du mir auf diese Nummer auf WhatsApp schreiben?»\n",
    "\n",
    "Sie habe sich nicht viel überlegt und das gemacht, sagt Egli, Mutter zweier erwachsener Söhne.\n",
    "\n",
    "«Mami, mir ist es schaurig peinlich, aber ich habe ein Anliegen. Ich schäme mich etwas, dich zu fragen.»\n",
    "\n",
    "Dann habe, so Egli, die Person nochmals in zwei Nachrichten in diese Richtung rumgedruckst. Sie habe geantwortet, sie solle endlich sagen, was los sei.\n",
    "\n",
    "«Ich muss zwei Rechnungen zahlen. Ich gebe dir das Geld sofort am Montag zurück.»\n",
    "\n",
    "Egli habe nachgefragt, um wie viel Geld es gehe.\n",
    "\n",
    "«Einmal 4400 Franken. Und einmal 3600 Franken. Ah, und Mami, ich wäre froh, wenn es unter uns bleiben könnte. Weil es mir eben peinlich ist.»\n",
    "\n",
    "Da sei sie das erste Mal stutzig geworden. «Das ist viel Geld. Zwar veranstaltete einer meiner Söhne zu dieser Zeit gerade ein Pop-up-Restaurant mit Freunden. Dass er Rechnungen zahlen musste, war realistisch», sagt Egli. Doch diese Art, nach Geld zu fragen, habe nicht zu ihrem Sohn gepasst. Egli wollte die Sache zuerst mit dem Ehemann besprechen. Darum schlug sie dem vermeintlichen Sohn vor, zu telefonieren – worauf nur zurückkam:\n",
    "\n",
    "«Wie viel Geld könntest du mir denn überweisen?»\n",
    "\n",
    "Als Eglis Mann von der Geschichte erfuhr, habe er sofort auf die echte Nummer des Sohnes angerufen. «Zum Glück hat dieser abgenommen, und so ist der Schwindel aufgeflogen.» Egli hat die Nummer blockiert, bei WhatsApp gemeldet und schliesslich gelöscht.\n",
    "\n",
    "Am nächsten Morgen kam eine Nachricht. Wieder von einer unbekannten Nummer.\n",
    "\n",
    "«Mami?»\n",
    "\n",
    "Diese neue Nachricht hat Egli wiederum sofort gemeldet, den Kontakt blockiert und gelöscht.\n",
    "\n",
    "Noch heute macht es Egli nachdenklich, wenn sie an den Vorfall zurückdenkt. «Das war schon dreist, ich war ziemlich erschüttert», sagt sie.\n",
    "Deutlich mehr Schaden\n",
    "\n",
    "Was Egli erlebt hat, passt in ein Muster. Beinahe vier von fünf Menschen über 55 haben laut einer aktuellen Studie von Pro Senectute in den letzten fünf Jahren einen Betrugsversuch erlebt. Knapp ein Fünftel der Befragten fielen gemäss der Erhebung einem Betrug zum Opfer.\n",
    "\n",
    "Am meisten überrascht hat Peter Burri Follath von Pro Senectute: «Uns ist die deutliche Erhöhung der jährlichen Schadenssumme um zwei Drittel gegenüber 2018 aufgefallen – von 400 auf 675 Millionen Franken», sagt er zu watson.\n",
    "Bild\n",
    "\n",
    "Er stelle eine Diskrepanz zwischen der öffentlichen Wahrnehmung und den effektiv begangenen Straftaten fest: In den Medien sei oft von Enkeltrick und falschen Polizisten zu lesen.\n",
    "«Von wesentlich höherer Bedeutung sind jedoch die Betrugssummen bei Delikten, bei denen eine Abhängigkeit zu professionellen oder nahestehenden Personen besteht.»\n",
    "Peter Burri Follath, Mediensprecher Pro Senectute\n",
    "\n",
    "Am meisten Schaden verursache der Missbrauch einer Vertrauensbeziehung durch eine Fachkraft. Und der Missbrauch einer Vollmacht beziehungsweise des Zugangs zu einem Bankkonto.\n",
    "Was tun?\n",
    "\n",
    "Kübli von der Schweizerischen Kriminalprävention empfiehlt genau das, was Eglis getan haben: «Das Telefon auflegen und bei den Angehörigen nachfragen.»\n",
    "\n",
    "Und beim Fall 1 der 34-Jährigen und dem falschen Interpol-Mitarbeiter rät sie, sich nicht unter Druck setzen zu lassen. Weiter gelte es, betrügerische Anrufe zu unterbrechen. Und: niemals Geld zu überweisen oder an der Türe zu übergeben.\n",
    "\n",
    "Zuletzt sei es wichtig, solche Anrufe – und auch Betrugsversuche – umgehend der Polizei zu melden.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#verfassung = verfassung.decode('utf-8')\n",
    "artikel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In einzelne Wörter + Zeichen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = nltk.word_tokenize(artikel)\n",
    "tokens[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In NUR Wörter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"#\".isalpha()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "words = word_tokenize(artikel)\n",
    "words=[word.lower() for word in words if word.isalpha()]\n",
    "words[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"9\".isalnum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hier noch ein paar praktische Python funktionen wie isalpha\n",
    "\n",
    "|  s.startswith(t) | test if s starts with t |\n",
    "|  ------ | ------ |\n",
    "|  s.endswith(t) | test if s ends with t |\n",
    "|  t in s | test if t is a substring of s |\n",
    "|  s.islower() | test if s contains cased characters and all are lowercase |\n",
    "|  s.isupper() | test if s contains cased characters and all are uppercase |\n",
    "|  s.isalpha() | test if s is non-empty and all characters in s are alphabetic |\n",
    "|  s.isalnum() | test if s is non-empty and all characters in s are alphanumeric |\n",
    "|  s.isdigit() | test if s is non-empty and all characters in s are digits |\n",
    "|  s.istitle() | test if s contains cased characters and is titlecased (i.e. all words in s have initial capitals) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In Sätze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "sentences = sent_tokenize(artikel)\n",
    "sentences[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming\n",
    "- d.h. das überführen von Wörtern in den gleichen Stamm. \n",
    "- Übliche Probleme die Sprache so mit sich bringt:\n",
    "    - Deklination: gehe, gehst, gehen, geht, ... es handelt sich ja quasi das gleiche Wort - uns sind diese Deklinationen egal. \n",
    "    - Plural, Groß/Kleinschreibung: katze, katzen, Katze ... es handelt sich auch um das gleiche Wort. \n",
    "    - ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter = nltk.PorterStemmer()\n",
    "lancaster = nltk.LancasterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Porter stemmer\n",
    "- geht ganz ok für Englisch\n",
    "- https://de.wikipedia.org/wiki/Porter-Stemmer-Algorithmus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import udhr\n",
    "engl_words = udhr.words(\"English-Latin1\")\n",
    "engl_words[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter.stem(\"declaration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter.stem(\"declaring\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter.stem(\"declare\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[porter.stem(w) for w in engl_words][0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lancaster Stemmer\n",
    "- kommt zu leicht anderen \"stämmen\"\n",
    "- funktioniert nicht so gut für Deutsch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[lancaster.stem(w) for w in engl_words][0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Snowball stemmer\n",
    "- Geht okish für deusch\n",
    "- http://snowball.tartarus.org/algorithms/german/stemmer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snowball = nltk.stem.snowball.GermanStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[snowball.stem(w) for w in words][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[lancaster.stem(w) for w in words][0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatizer\n",
    "- Zurückführen des Wortes zu \"Wörterbuchdefinitionen\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Women in technologies areas are amazing at coding. Especially one woman is great, her name is Ursula Burns.\"\n",
    "words = word_tokenize(text)\n",
    "words_engl =[word.lower() for word in words if word.isalpha()]\n",
    "(\" \").join(words_engl[0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemma = nltk.WordNetLemmatizer() \n",
    "lemmas = [lemma.lemmatize(i) for i in words_engl]\n",
    "(\" \").join(lemmas[0:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lematizer Deutsch NLTK\n",
    "- Geht leider nicht für deusch :( in nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "party_text = '''\n",
    "Kurz vor 22.00 Uhr sind Polizisten der Stadtpolizei in alle Richtungen aber vor allem in Richtung Kreis 9 ausgerückt. Grund war eine Meldung zu einer privaten Party an der Aargauerstrasse, die angeblich ausser Kontrolle geraten sei. \n",
    "'''\n",
    "#party_text = party_text.decode(\"UTF-8\")\n",
    "words = word_tokenize(party_text)\n",
    "words_german =[word.lower() for word in words if word.isalpha()]\n",
    "(\" \").join(words_german[0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma = nltk.WordNetLemmatizer() \n",
    "lemmas = [lemma.lemmatize(i) for i in words_german]\n",
    "(\" \").join(lemmas[0:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatizer in Textblob\n",
    " - https://textblob.readthedocs.io/en/dev/quickstart.html\n",
    " - ```pip install -U textblob```\n",
    " - ```pip install -U textblob-de```\n",
    " - https://github.com/markuskiller/textblob-de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -U textblob --user\n",
    "#!pip install -U --upgrade textblob-de --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob_de import TextBlobDE\n",
    "from textblob_de import PatternParser\n",
    "\n",
    "party_text = '''Kurz vor 22.00 Uhr sind Polizisten der Stadtpolizei in alle Richtungen aber vor allem in Richtung Kreis 9 ausgerückt. Grund war eine Meldung zu einer privaten Party an der Aargauerstrasse, die angeblich ausser Kontrolle geraten sei. \n",
    "'''\n",
    "blob = TextBlobDE(party_text, parser=PatternParser(pprint=False, lemmata=True))\n",
    "blob.parse()\n",
    "list(blob.words.lemmatize())[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spacy \n",
    "## Spacy German Lemmatizer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Spacy ist ein machine learning natural language processing tool. \n",
    "- Aufgrund des Deep Learning Ansatzes ist es am weitesten von allen derzeitigen Tools fortgeschritten. \n",
    "- Allerdings erschliessen sich viele Funktionen für Laien nicht sofort. Wir werden uns spacy morgen im detail anschauen. \n",
    "\n",
    "- ```pip install --user spacy```\n",
    "- Deutsche Modelle runterladen:```pip install https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-2.0.0/de_core_news_sm-2.0.0.tar.gz```\n",
    "- oder ```python -m spacy download de_core_news_sm```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install spacy\n",
    "#!conda install -c conda-forge spacy -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!conda install nomkl -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 23.7.4\n",
      "  latest version: 23.9.0\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c conda-forge conda\n",
      "\n",
      "Or to minimize the number of packages updated during conda update use\n",
      "\n",
      "     conda install conda=23.9.0\n",
      "\n",
      "\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /Users/plotti/miniconda3/envs/chat\n",
      "\n",
      "  added / updated specs:\n",
      "    - spacy-model-de_core_news_sm\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    spacy-model-de_core_news_sm-3.1.0|   py39h5ab759e_0        18.6 MB  conda-forge\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:        18.6 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  python_abi         conda-forge/osx-64::python_abi-3.9-2_cp39 \n",
      "  spacy-model-de_co~ conda-forge/osx-64::spacy-model-de_core_news_sm-3.1.0-py39h5ab759e_0 \n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "  ca-certificates    pkgs/main::ca-certificates-2023.01.10~ --> conda-forge::ca-certificates-2023.7.22-h8857fd0_0 \n",
      "  certifi            pkgs/main/osx-64::certifi-2023.5.7-py~ --> conda-forge/noarch::certifi-2023.7.22-pyhd8ed1ab_0 \n",
      "  openssl              pkgs/main::openssl-1.1.1t-hca72f7f_0 --> conda-forge::openssl-1.1.1w-h8a1eda9_0 \n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "                                                                                \n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda install -c conda-forge spacy-model-de_core_news_sm -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install treetaggerwrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download de_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'de_core_news_sm'. It doesn't seem to be a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m nlp \u001b[38;5;241m=\u001b[39m \u001b[43mspacy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mde_core_news_sm\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m doc \u001b[38;5;241m=\u001b[39m nlp(\u001b[38;5;124m'''\u001b[39m\u001b[38;5;124mKurz vor 22.00 Uhr sind Polizisten der Stadtpolizei in alle Richtungen aber vor allem in Richtung Kreis 9 ausgerückt. Grund war eine Meldung zu einer privaten Party an der Aargauerstrasse, die angeblich ausser Kontrolle geraten sei. \u001b[39m\u001b[38;5;124m'''\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#doc = nlp('Die Schweizer Wirtschaft dürfte 2020 ein Wachstum von zwei Prozent verzeichnen, sagen die Datenschutzbeauftragten. '          'Dasselbe gilt für Versicherungsprämien und Aufwände der Krankenkassen im Allgemeinen. Auch die Bäume haben ein Problem.')\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/chat/lib/python3.9/site-packages/spacy/__init__.py:50\u001b[0m, in \u001b[0;36mload\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\n\u001b[1;32m     27\u001b[0m     name: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     33\u001b[0m     config: Union[Dict[\u001b[38;5;28mstr\u001b[39m, Any], Config] \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39mSimpleFrozenDict(),\n\u001b[1;32m     34\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Language:\n\u001b[1;32m     35\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load a spaCy model from an installed package or a local path.\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \n\u001b[1;32m     37\u001b[0m \u001b[38;5;124;03m    name (str): Package name or model path.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;124;03m    RETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m        \u001b[49m\u001b[43menable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/chat/lib/python3.9/site-packages/spacy/util.py:472\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m    470\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m OLD_MODEL_SHORTCUTS:\n\u001b[1;32m    471\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE941\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname, full\u001b[38;5;241m=\u001b[39mOLD_MODEL_SHORTCUTS[name]))  \u001b[38;5;66;03m# type: ignore[index]\u001b[39;00m\n\u001b[0;32m--> 472\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE050\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname))\n",
      "\u001b[0;31mOSError\u001b[0m: [E050] Can't find model 'de_core_news_sm'. It doesn't seem to be a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"de_core_news_sm\")\n",
    "doc = nlp('''Kurz vor 22.00 Uhr sind Polizisten der Stadtpolizei in alle Richtungen aber vor allem in Richtung Kreis 9 ausgerückt. Grund war eine Meldung zu einer privaten Party an der Aargauerstrasse, die angeblich ausser Kontrolle geraten sei. ''')\n",
    "#doc = nlp('Die Schweizer Wirtschaft dürfte 2020 ein Wachstum von zwei Prozent verzeichnen, sagen die Datenschutzbeauftragten. '          'Dasselbe gilt für Versicherungsprämien und Aufwände der Krankenkassen im Allgemeinen. Auch die Bäume haben ein Problem.')\n",
    "lemmas = [token.lemma_ for token in doc]\n",
    "print(lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS Tagging (Part of Speech Tagging)\n",
    "- Ich bin nur interessiert an Substantiven oder Adjektiven oder Verben. \n",
    "- Wie kann ich nur diese filtern? \n",
    "- Funktioniert für Deutsch nicht auf Anhieb in NLTK :(\n",
    "- aber gut in spacy und textblob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Übersicht\n",
    "https://www.clips.uantwerpen.be/pages/mbsp-tags\n",
    "\n",
    "|  Tag | Description | Example |\n",
    "|  ------ | ------ | ------ |\n",
    "|  **CC** | conjunction, coordinating | and, or, but |\n",
    "|  **CD** | cardinal number | five, three, 13% |\n",
    "|  **DT** | determiner | the, a, these |\n",
    "|  **EX** | existential there | there were six boys |\n",
    "|  **FW** | foreign word | mais |\n",
    "|  **IN** | conjunction, subordinating or preposition | of, on, before, unless |\n",
    "|  **JJ** | adjective | nice, easy |\n",
    "|  **JJR** | adjective, comparative | nicer, easier |\n",
    "|  **JJS** | adjective, superlative | nicest, easiest |\n",
    "|  **LS** | list item marker |  |\n",
    "|  **MD** | verb, modal auxillary | may, should |\n",
    "|  **NN** | noun, singular or mass | tiger, chair, laughter |\n",
    "|  **NNS** | noun, plural | tigers, chairs, insects |\n",
    "|  **NNP** | noun, proper singular | Germany, God, Alice |\n",
    "|  **NNPS** | noun, proper plural | we met two Christmases ago |\n",
    "|  **PDT** | predeterminer | both his children |\n",
    "|  **POS** | possessive ending | s |\n",
    "|  **PRP** | pronoun, personal | me, you, it |\n",
    "|  **PRP** | pronoun, possessive | my, your, our |\n",
    "|  **RB** | adverb | extremely, loudly, hard |\n",
    "|  **RBR** | adverb, comparative | better |\n",
    "|  **RBS** | adverb, superlative | best |\n",
    "|  **RP** | adverb, particle | about, off, up |\n",
    "|  **SYM** | symbol | % |\n",
    "|  **TO** | infinitival to | what to do? |\n",
    "|  **UH** | interjection | oh, oops, gosh |\n",
    "|  **VB** | verb, base form | think |\n",
    "|  **VBZ** | verb, 3rd person singular present | she thinks |\n",
    "|  **VBP** | verb, non-3rd person singular present | I think |\n",
    "|  **VBD** | verb, past tense | they thought |\n",
    "|  **VBN** | verb, past participle | a sunken ship |\n",
    "|  **VBG** | verb, gerund or present participle | thinking is fun |\n",
    "|  **WDT** | wh-determiner | which, whatever, whichever |\n",
    "|  **WP** | wh-pronoun, personal | what, who, whom |\n",
    "|  **WP** | wh-pronoun, possessive | whose, whosever |\n",
    "|  **WRB** | wh-adverb | where, when |\n",
    "|  **.** | punctuation mark, sentence closer | .;?* |\n",
    "|  **,** | punctuation mark, comma | , |\n",
    "|  **:** | punctuation mark, colon | : |\n",
    "|  **(** | contextual separator, left paren | ( |\n",
    "|  **)** | contextual separator, right paren | ) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS Tags in spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "party_text = '''Kurz vor 22.00 Uhr sind Polizisten der Stadtpolizei in alle Richtungen aber vor allem in Richtung Kreis 9 ausgerückt. Grund war eine Meldung zu einer privaten Party an der Aargauerstrasse, die angeblich ausser Kontrolle geraten sei. \n",
    "'''\n",
    "party_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verben = []\n",
    "substantive = []\n",
    "adjektive = []\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"de_core_news_sm\")\n",
    "doc = nlp(party_text)\n",
    "\n",
    "for token in doc:\n",
    "    if \"VERB\" in token.pos_:\n",
    "        verben.append(token)\n",
    "    if \"NOUN\" in token.pos_:\n",
    "        substantive.append(token)\n",
    "    if \"ADJ\" in token.pos_:\n",
    "        adjektive.append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verben"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "substantive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjektive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS tags in textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "substantive = []\n",
    "adjektive = []\n",
    "verben = []\n",
    "blob = TextBlobDE(party_text, parser=PatternParser(pprint=False, lemmata=True))\n",
    "for word in blob.tags:\n",
    "    if \"NN\" in word[1]:\n",
    "        substantive.append(word[0])\n",
    "    if \"V\" in word[1]:\n",
    "        verben.append(word[0])\n",
    "    if \"JJ\" in word[1]:\n",
    "        adjektive.append(word[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "substantive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjektive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verben"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop word removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Englisch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_engl = [\"the\", \"big\", \"tree\", \"and\", \"the\", \"duck\"]\n",
    "filtered_words = [word for word in words_engl if word not in stopwords.words('english')]\n",
    "filtered_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deutsch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mylist = stopwords.words('german')\n",
    "#mylist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eigene Stopwords hinzufügen oder entfernen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "mylist = stopwords.words('german')\n",
    "mylist.append(\"plotti\")\n",
    "mylist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [\"plotti\", \"was\", \"was\", \"here\"]\n",
    "a.remove(\"plotti\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "my_new_list = list(set(mylist)-set([\"ich\"]))\n",
    "#my_new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mylist = stopwords.words('german')\n",
    "mylist.append(\"Seite\")\n",
    "mylist.append(\"test\")\n",
    "mylist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_german = [\"ich\", \"war\", \"im\", \"wald\", \"spazieren\", \"Seite\", \"test\",\"plotti\"]\n",
    "filtered_words = [word for word in words_german if word not in mylist]\n",
    "filtered_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wordle 2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tokenization\n",
    "- Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = (\" \").join(udhr.words(\"German_Deutsch-Latin1\"))\n",
    "#text\n",
    "# tokenize \n",
    "words = word_tokenize(text)\n",
    "#words\n",
    "# nur wörter behalten\n",
    "words_german =[word.lower() for word in words if word.isalpha()]\n",
    "words_german"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatize\n",
    "lemma = nltk.WordNetLemmatizer()\n",
    "lemmas = [lemma.lemmatize(i) for i in words_german]\n",
    "clean_blob = lemmas\n",
    "#\" \".join(clean_blob[:30])\n",
    "clean_blob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Numbers etc.. removal\n",
    "- Stop word removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install textacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textacy\n",
    "ngrams = list(textacy.extract.basics.ngrams(doc, 2, min_freq=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "from os import path\n",
    "\n",
    "text = \"\"\"Letzte Chance für Jositsch? SVP und SP entscheiden über Kandidaturen\n",
    "Heute fallen wichtige Vorentscheide im Hinblick auf die Bundesratswahl. Die SVP-Fraktion nominiert ihre Kandidaten und die SP bestimmt, ob ein Mann für das Ticket infrage kommt.\n",
    "7\n",
    "18.11.2022, 05:59\n",
    "Peter Blunschi\n",
    "Peter Blunschi\n",
    "Peter Blunschi\n",
    "Folge mir\n",
    "Mehr «Schweiz»\n",
    "Kantonsspital Aarau wird selbst zum Notfall: 240 Millionen Franken Finanzhilfe gefordert\n",
    "Klimaaktivisten stehen wegen Banken-Blockade vor Zürcher Gericht\n",
    "Von der Rolle: Das WC-Papier dürfte deutlich teurer werden\n",
    "Maurer warnt: «Es geht einfach nicht mehr auf»\n",
    "Promotion\n",
    "Leasing oder kaufen? Warum Jeffrey mit dem Auto-Abo besser fährt.\n",
    "Promotion\n",
    "Gewinne eine 2-wöchige Reise im MINI Countryman\n",
    "\n",
    "In weniger als drei Wochen steht fest, wer Ueli Maurer (SVP) und Simonetta Sommaruga (SP) im Bundesrat ersetzen wird. Nun gilt es ernst für die Bewerberinnen und Bewerber um die Nachfolge. Am Freitag treffen sich die Bundeshausfraktionen beider Parteien, um wichtige Vorentscheide zu fällen. Die Bundesratswahl kommt in ihre heisse Phase.\n",
    "SVP\n",
    "1 / 8\n",
    "Maurer-Nachfolge: Diese 5 Kandidaten wollen in den Bundesrat\n",
    "Das Kandidatenkarussell für die Nachfolge von SVP-Bundesrat Ueli Maurer dreht sich seit dessen Rücktrittsankündigung Ende September. Die wichtigsten Namen im Überblick:\n",
    "quelle: keystone / peter klaunzer\n",
    "Auf Facebook teilen\n",
    "Auf Twitter teilen\n",
    "Whatsapp sharer\n",
    "\n",
    "Die SVP-Fraktion tagt in Hérémance im Kanton Wallis. Sie dürfte für den Maurer-Sitz ein Zweierticket nominieren und damit der Empfehlung von Findungskommission und Fraktionsvorstand folgen. Auf Namen wollten sich beide Gremien nicht festlegen. Alle fünf Kandidierenden hätten einen eindrücklichen Leistungsausweis und seien wählbar, hiess es.\n",
    "\n",
    "Zur Wahl stellen sich:\n",
    "\n",
    "    Michèle Blöchliger, Regierungsrätin (Nidwalden)\n",
    "    Albert Rösti, Nationalrat (Bern)\n",
    "    Werner Salzmann, Ständerat (Bern)\n",
    "    Heinz Tännler, Regierungsrat (Zug)\n",
    "    Hans-Ueli Vogt, alt Nationalrat (Zürich)\n",
    "\n",
    "Sie können sich in einem Hearing präsentieren, in alphabetischer Reihenfolge, und erhalten je fünf Minuten Zeit, sich vorzustellen. Anschliessend können ihnen die Fraktionsmitglieder Fragen stellen. Danach wird die Fraktion die Nomination vornehmen. Über den Entscheid wird gemäss einer Mitteilung nicht vor 17.30 Uhr informiert.\n",
    "\n",
    "Als gesetzt gilt Rösti. Der frühere SVP-Präsident verfügt über starken Rückhalt in der Fraktion und kommt auch bei den anderen Parteien gut an. Um den zweiten Platz auf dem Ticket könnte es zu einem «Hosenlupf» zwischen Tännler und Vogt kommen. Unter Röstis «Getreuen» gibt es angeblich Bestrebungen, den homosexuellen Zürcher zu verhindern.\n",
    "\n",
    "Solche Spekulationen und Machtspielchen sind Teil fast jeder Bundesratswahl. Sie sind nur bedingt ernst zu nehmen. Albert Rösti jedenfalls ist und bleibt Topfavorit. Werner Salzmann hingegen wird es als zweiter Berner kaum auf das Ticket schaffen. Und Michèle Blöchliger dürfte über ihre schweizerisch-britische Doppelbürgerschaft stolpern.\n",
    "SP\n",
    "1 / 6\n",
    "Sommaruga-Nachfolge: Diese Kandidatinnen und Kandidaten wollen in den Bundesrat\n",
    "Evi Allemann ist 44, sass bis 2018 im Nationalrat für den Kanton Bern und amtet für diesen nun als Justizdirektorin in der Regierung.\n",
    "quelle: keystone / peter schneider\n",
    "Auf Facebook teilen\n",
    "Auf Twitter teilen\n",
    "Whatsapp sharer\n",
    "\n",
    "Bei den Sozialdemokraten, die für die Ersatzwahl weniger Zeit zur Verfügung haben als die SVP, geht es am Freitag noch nicht um Namen. Die Fraktion entscheidet im Bundeshaus vielmehr über die Kriterien, die für eine Kandidatur gelten sollen. Konkret geht es um das von Partei- und Fraktionspräsidium vorgeschlagene Ticket mit zwei Frauen.\n",
    "\n",
    "Die Ausgrenzung der Männer bei gleichzeitiger Öffnung für Bewerbungen aus allen Landesteilen hat für Diskussionsstoff und Unmut gesorgt. Provoziert fühlt sich vor allem der Zürcher Ständerat Daniel Jositsch, dem schon länger Bundesratsambitionen nachgesagt werden. Er hat trotz der Ausschlusskriterien seine Kandidatur eingereicht.\n",
    "\n",
    "Die Sitzung am Freitag ist wohl die letzte Chance für Jositsch, es doch auf das Ticket für die Sommaruga-Nachfolge zu schaffen. Sein Solothurner Ratskollege Roberto Zanetti schlägt eine Dreiernominierung vor. Dazu wird es jedoch kaum kommen. Es ist sehr wahrscheinlich, dass die SP-Fraktion trotz der teils heftigen Kritik am reinen Frauenticket festhalten wird.\n",
    "\n",
    "Zur Wahl stellen sich:\n",
    "\n",
    "    Evi Allemann, Regierungsrätin (Bern)\n",
    "    Elisabeth Baume-Schneider, Ständerätin (Jura)\n",
    "    Eva Herzog, Ständerätin (Basel-Stadt)\n",
    "\n",
    "Beim heutigen Stand zeichnet sich ein Ticket mit Allemann und Herzog ab, wobei die Baslerin als Favoritin gilt. Baume-Schneider dürfte als Westschweizerin kaum eine Chance haben. Allerdings läuft die Meldefrist bis kommenden Montag um 12 Uhr. Eine kurzfristige weitere Kandidatur ist nicht auszuschliessen, aber wenig wahrscheinlich.\n",
    "SP-Parteiratschefin nach Jositsch-Zoff: «Werden alle Kandidaturen vorurteilslos prüfen»\n",
    "\n",
    "Danach wird eine Findungskommission die eingereichten Kandidaturen auf ihre Eignung hin prüfen. Parallel dazu finden vier öffentliche Hearings statt, an denen sich die Kandidierenden den Parteimitgliedern und interessierten Personen vorstellen:\n",
    "\n",
    "    21. November, 19 Uhr, Neubad, Luzern\n",
    "    22. November, 19 Uhr, Centre Pluriculturel et social d’Ouchy, Lausanne\n",
    "    23. November, 19 Uhr, Volkshaus, Zürich\n",
    "    24. November, 19 Uhr, Kulturhotel Guggenheim, Liestal\n",
    "\n",
    "Am 25. November gibt der Parteirat der SP Schweiz an einer ausserordentlichen Sitzung eine Empfehlung zuhanden der Bundeshausfraktion ab. Diese wird tags darauf erneut zusammenkommen, um das endgültige Ticket zu nominieren. Am 28. November beginnt die Wintersession mit Hearings in allen Fraktionen, und am 7. Dezember findet die Wahl statt.\n",
    "Mehr zu den Bundesratswahlen:\n",
    "\n",
    "    SP Kanton Bern nominiert Evi Allemann als Bundesratskandidatin\n",
    "    «Wirkt undemokratisch»: Alt Bundesrat Leuenberger kritisiert SP für reines Frauenticket\n",
    "    Die Grünen machen Ernst mit der Bundesratskandidatur\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "### vorverarbeiten mit nltk\n",
    "words = word_tokenize(text)\n",
    "words_alpha =[word.lower() for word in words if word.isalpha()]\n",
    "stopwords = stopwords.words('german')\n",
    "stopwords.append(\"uhr\")\n",
    "stopwords.append(\"november\")\n",
    "result = [word for word in words_alpha if word not in stopwords]\n",
    "text = (\" \").join(result)\n",
    "\n",
    "### Rest mit spacy\n",
    "#v1\n",
    "#doc = nlp(text)\n",
    "#substantive = []\n",
    "#lemmas = [str(token.lemma_) for token in doc]\n",
    "#text = (\" \").join(lemmas)\n",
    "\n",
    "d = path.dirname(__file__) if \"__file__\" in locals() else os.getcwd()\n",
    "\n",
    "maske = np.array(Image.open(path.join(d, \"maske.png\")))\n",
    "\n",
    "wc = WordCloud(collocation_threshold=1,background_color=\"white\", max_words=2000, mask=maske, contour_width=3, contour_color='steelblue',collocations=True, normalize_plurals=False)\n",
    "wc.generate(text)\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.imshow(wc, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#v2\n",
    "#for token in doc:\n",
    "#    if \"NOUN\" in token.pos_:\n",
    "#        substantive.append(str(token))\n",
    "#text = (\" \").join(substantive)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Wenn man viele files hat und jedes verarbeiten möchte, kann man daraus eine funktion machen.  Wie z.B. hier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "def process_text(text):\n",
    "    words = word_tokenize(text)\n",
    "    words_german =[word.lower() for word in words if word.isalpha()]\n",
    "    lemma = nltk.WordNetLemmatizer()\n",
    "    lemmas = [lemma.lemmatize(i) for i in words_german]\n",
    "    result = [word for word in lemmas if word not in stopwords]\n",
    "    return result\n",
    "\n",
    "#for file in files:\n",
    "#    result.append(process_text(file))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chat",
   "language": "python",
   "name": "chat"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
